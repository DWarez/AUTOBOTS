# AUTOBOTS ðŸ¤–
My implementation of Transformers from the paper "Attention is all you need"

### About the repository
This repository has been made only for educational purposes. This implementation was realized only for my personal better understanding of the Transformer model.

### Useful resources
**Token Embedding**: [AssemblyAI's explanation on word embeddings (YouTube)](https://www.youtube.com/watch?v=5MaWmXwxFNQ)

**Positional Encoding**: [Amirhossein Kazemnejad's explanation](https://kazemnejad.com/blog/transformer_architecture_positional_encoding/)


**Layer Normalization**: [Papers with code](https://paperswithcode.com/method/layer-normalization)

**Attention**: [Jay Alammar's explanation](https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/)


**Transformers**: [Jay Alammar's "The illustrated transformer](https://jalammar.github.io/illustrated-transformer/), [Yannic Kilcher explanation of "Attention is all you need" (YouTube)](https://www.youtube.com/watch?v=iDulhoQ2pro), [AI coffee break with Letitia's explanation (Youtube)](https://www.youtube.com/watch?v=FWFA4DGuzSc)


**Other implementations**: [Ben Trevett's Seq2Seq notebook](https://github.com/bentrevett/pytorch-seq2seq/blob/master/6%20-%20Attention%20is%20All%20You%20Need.ipynb), [The annotated Transformer](http://nlp.seas.harvard.edu/annotated-transformer/#positional-encoding), [hyunwoongko's implementation](https://github.com/hyunwoongko/transformer), [SamLynnEvans' implementation](https://github.com/SamLynnEvans/Transformer)